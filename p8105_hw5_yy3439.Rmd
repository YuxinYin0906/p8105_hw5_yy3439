---
title: "p8105_hw5_yy3439"
output: github_document
date: "2023-11-14"
---
```{r}
library(tidyverse)
library(ggplot2)
library(purrr)
library(broom)
```



## Problem 1

```{r}
homicide_data = read_csv("./data/homicide-data.csv") #import data
```

### Description of the Raw Data

The raw data has 12 columns and 52179 rows. The variables(columns) are comprised of two types: characters and numerical.

### Total Number of homicides and the number of unsolved homicides
```{r}
homicide_data_tidy = homicide_data|>
  mutate(city_state = str_c(city, state, sep = ", ", collapse = NULL))|> #create the city_state variable
  mutate(result = case_when(
    disposition == "Closed without arrest" ~ "unsolved", #create the result variable which indicates whether the case is solved or unsolved
    disposition == "Closed by arrest" ~ "solved",
    disposition == "Open/No arrest" ~ "unsolved"
  ))
```

```{r}
homicides_sum = homicide_data_tidy|>
  group_by(city_state)|>
  summarise(total_homicides = n(),
            unsolved_homicides = sum(result == "unsolved"))
homicides_sum
```

As a result, the data frame contains each city in the U.S. and their corresponding totoal number of homicides as well as total number of unsolved homicides. 

### Conduct `prop.test` on Baltimore

```{r}
Baltimore_hom_total = homicides_sum|>
  filter(city_state == "Baltimore, MD")|>
  pull(total_homicides)
Baltimore_hom_unsolved = homicides_sum|>
  filter(city_state == "Baltimore, MD")|>
  pull(unsolved_homicides)

baltimore_test = prop.test(Baltimore_hom_unsolved, Baltimore_hom_total)
baltimore_test_tidy = broom::tidy(baltimore_test)
```
As a result, the estimated proportion of unsolved homicides in Baltimore is `r baltimore_test_tidy |> pull(estimate)`. In addition, the confidence interval is [`r baltimore_test_tidy |> pull(conf.low)`, `r baltimore_test_tidy |> pull(conf.high)`]. Additionally, the estimated proportion falls between the confidence interval. 

### Conduct the `prop.test` On Each City

```{r}
test = homicides_sum |>
  mutate(
  prop_test = map2(homicides_sum$unsolved_homicides, homicides_sum$total_homicides, \(x, y) prop.test(x = x, n = y)),
  tidy_test = map(prop_test, broom::tidy))|>
  select(-prop_test)|>
  unnest(tidy_test)|>
  select(city_state, estimate, conf.low, conf.high)
test
```

### Visualization

```{r}
test|>
  mutate(city_state = fct_reorder(city_state, estimate))|>
  ggplot(aes(x = city_state, y = estimate))+
  geom_point()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high))+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Problem 2

### Step 1

Ceate a dataframe containing all file names.

```{r}
filenames_data = list.files(path = "data_p2/") #import data
```

### Step 2

Then, we iterate file names and read in data for each subject using `purrr::map` and saving the result as a new variable in the dataframe. Additionally, we add each participant's id as the first entry column. 

```{r, message = FALSE}
files_df = map_dfr(filenames_data, ~read_csv(file.path("data_p2/", .)), .id = "id")|>
  mutate(id = as.numeric(id))|>
  bind_rows()
```



### Step 3

Finally, we should tidy the `files_df` data frame and include the control arm and subject ID.

```{r}
files_df_tidy = files_df|>
  mutate(arm = case_when(
    id < 11 ~ "Control",
    id >10 ~ "Experiment"
  ),
  subject_id = ifelse(
    id < 11, id, id-10
  )) |>
  pivot_longer(
    cols = starts_with("week"),
    names_to = "week",
    values_to = "values",
    names_prefix = "week_"
  )|>
  select(-id)|>
  relocate(subject_id, arm)|>
  mutate(subject_id = as.character(subject_id))

```
The above data frame containing subject id for both control and experiment groups as well as each week's values for every participants. It has `r nrow(files_df_tidy)` rows and `r ncol(files_df_tidy)` columns. 

### Visualization

Then, we would create a spaghetti plot showing observations on each subject over time. 

```{r sphaghetti_plot_p2}
spaghetti_p = ggplot(files_df_tidy, aes(x = week, y = values, group = subject_id))
spaghetti_p+
  geom_line(aes(color = arm))+
  stat_summary(aes(group = 1), geom = "point", fun.y = mean,
    shape = 17, size = 2)+
  facet_grid(~arm)+
  labs(title = "Observations On Each Subject Over 8-weeks Period")+
  xlab("Week")+
  ylab("Value")
  
  
```
Based on the above graph, we can see that the mean values(indicated by the black triangle points) of the control group is generally more stable and are less than their corresponding experiment groups for each week. Conversely, the mean value of the experiment group increases almost every week from week 1 to week 8. 

## Problem 3

First, set the seed so that our results would be reproducible.
```{r}
set.seed(1) #so that the results are reproducible 
```

Then, we generate 5000 datasets from the model.

```{r}
output = vector("list", 5000)

for (i in 1: 5000){
  dataset = rnorm(n = 30, mean = 0, sd = 5)
  output[[i]] = dataset
}
```

Next, for each dataset, save mu_hat and the p-value.

```{r}
t_test = function(data){
  t.test(data, conf.level = 0.95)|>
    broom::tidy()|>
    select(estimate, p.value)
}

#when the true mean is 0
t_test0 = map(output, t_test)|>
  bind_rows()
```
Then, we can use simulation to repeat the above process for true mean = {1,2,3,4,5,6}.

```{r}
simulation = function(true_mean){
  sim_data = tibble(
    x = rnorm(n = 30, mean = true_mean, sd = 5)
  )
  
  output = sim_data|>
    t.test()|>
    broom::tidy()|>
    select(estimate, p.value)
}

sim_results_df = 
  expand_grid(
    true_mean = c(1,2,3,4,5,6),
    iter = 1:5000
  ) |> 
  mutate(
    estimate = map(true_mean, simulation)
  ) |> 
  unnest(estimate)
  
```


### Visualization: Part 1

```{r visualization_p3_1}
sim_results_df|>
  group_by(true_mean)|>
  summarise(
    reject = sum(p.value < 0.05),
    reject_prop = reject/5000
  )|>
  ggplot(aes(x = true_mean, y = reject_prop))+
  geom_line()+
  scale_x_continuous(breaks = c(1:6))+
  labs(title = "Reject Proportion for Each True Mean")+
  xlab("True Mean")+
  ylab("Reject Proportion")
```
Based on the above graph, we can see that, as the true mean increases, the proportion of rejected null hypothesis increases. This association can be interpreted as larger effect size would increase the power of the study. 


